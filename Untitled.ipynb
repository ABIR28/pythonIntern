{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87a636a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install requests beautifulsoup4 transformers gitpython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6ff1d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\abir\\anaconda3\\lib\\site-packages (2.16.1)\n",
      "Requirement already satisfied: keras in c:\\users\\abir\\anaconda3\\lib\\site-packages (3.3.3)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\abir\\anaconda3\\lib\\site-packages (from tensorflow) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\abir\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\abir\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\abir\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\abir\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\abir\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\abir\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\abir\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\abir\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\abir\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\abir\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\abir\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\abir\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\abir\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\abir\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\abir\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\abir\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\abir\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\abir\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.63.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\abir\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\abir\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\abir\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: rich in c:\\users\\abir\\anaconda3\\lib\\site-packages (from keras) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\abir\\anaconda3\\lib\\site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\abir\\anaconda3\\lib\\site-packages (from keras) (0.11.0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\abir\\anaconda3\\lib\\site-packages (from rich->keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\abir\\anaconda3\\lib\\site-packages (from rich->keras) (2.15.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\abir\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\abir\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras) (0.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abir\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abir\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abir\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abir\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\abir\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\abir\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\abir\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\abir\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e43313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import json\n",
    "from transformers import pipeline\n",
    "\n",
    "# Set your Huggingface API key here\n",
    "HUGGINGFACE_API_KEY = \"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\"\n",
    "\n",
    "# Base URL to scrape\n",
    "base_url = \"https://www.jugantor.com/sports\"\n",
    "\n",
    "def get_filtered_urls(base_url, keyword):\n",
    "    \"\"\"\n",
    "    Scrape and filter URLs containing the specified keyword.\n",
    "    \"\"\"\n",
    "    response = requests.get(base_url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve {base_url}. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = soup.find_all('a', href=True)\n",
    "\n",
    "    # Filter URLs by keyword\n",
    "    filtered_urls = [urljoin(base_url, link['href']) for link in links if keyword in link['href']]\n",
    "    return filtered_urls\n",
    "\n",
    "def extract_data_from_url(url):\n",
    "    \"\"\"\n",
    "    Extracts all available data from the article page.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve {url}. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    data = {\n",
    "        \"url\": url,\n",
    "        \"title\": soup.title.string if soup.title else \"No Title\",\n",
    "        \"author\": extract_author(soup),\n",
    "        \"publication_date\": extract_publication_date(soup),\n",
    "        \"content\": extract_content(soup)\n",
    "    }\n",
    "\n",
    "    return data\n",
    "\n",
    "def extract_content(soup):\n",
    "    \"\"\"\n",
    "    Extracts the main content of the article.\n",
    "    \"\"\"\n",
    "    paragraphs = soup.find_all('p')\n",
    "    content = \" \".join(p.get_text() for p in paragraphs)\n",
    "    return content if content else \"No Content\"\n",
    "\n",
    "def extract_author(soup):\n",
    "    \"\"\"\n",
    "    Extracts the author from the article page, if available.\n",
    "    \"\"\"\n",
    "    author_tag = soup.find(class_=\"author-name\")\n",
    "    return author_tag.get_text().strip() if author_tag else \"Unknown Author\"\n",
    "\n",
    "def extract_publication_date(soup):\n",
    "    \"\"\"\n",
    "    Extracts the publication date from the article page.\n",
    "    \"\"\"\n",
    "    date_tag = soup.find(\"time\")\n",
    "    return date_tag[\"datetime\"] if date_tag and date_tag.has_attr(\"datetime\") else \"Unknown Date\"\n",
    "\n",
    "def analyze_with_llm(content):\n",
    "    \"\"\"\n",
    "    Uses Huggingface's pipeline to analyze sentiment, importance, and perspective.\n",
    "    \"\"\"\n",
    "    headers = {\"Authorization\": f\"Bearer {HUGGINGFACE_API_KEY}\"}\n",
    "    sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "    \n",
    "    # Analyze sentiment\n",
    "    sentiment_result = sentiment_pipeline(content[:512])[0]\n",
    "\n",
    "    # Generate importance score (example: based on content length)\n",
    "    importance_score = min(round(len(content) / 100, 2), 10.0)  # Capped at 10.0\n",
    "\n",
    "    # Analyze international relevance (dummy example using keywords)\n",
    "    international_relevance = \"High\" if any(kw in content.lower() for kw in [\"world\", \"international\", \"global\"]) else \"Low\"\n",
    "\n",
    "    return {\n",
    "        \"sentiment\": sentiment_result[\"label\"],\n",
    "        \"confidence\": sentiment_result[\"score\"],\n",
    "        \"importance_score\": importance_score,\n",
    "        \"international_relevance\": international_relevance\n",
    "    }\n",
    "\n",
    "def save_to_json(data, filename=\"scraped_data.json\"):\n",
    "    \"\"\"\n",
    "    Saves the data to a JSON file.\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def main():\n",
    "    # Scrape and filter URLs related to sports\n",
    "    sports_urls = get_filtered_urls(base_url, \"sports\")\n",
    "\n",
    "    # Extract and analyze data for each article\n",
    "    all_data = []\n",
    "    for i, url in enumerate(sports_urls, 1):\n",
    "        print(f\"Processing {i}/{len(sports_urls)}: {url}\")\n",
    "        data = extract_data_from_url(url)\n",
    "        if data:\n",
    "            analysis = analyze_with_llm(data[\"content\"])\n",
    "            data.update(analysis)  # Integrate LLM-generated analysis\n",
    "            all_data.append(data)\n",
    "\n",
    "    # Save the final data to JSON\n",
    "    save_to_json(all_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a748779f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
